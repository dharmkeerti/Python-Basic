{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Classification: Decision Trees, SVM, and Naive Bayes Assignment"
      ],
      "metadata": {
        "id": "busaCdHMt-6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1. What is Information Gain, and how is it used in Decision Trees?**\n",
        "- Information Gain measures how much uncertainty (or impurity) in the target variable is reduced after splitting the data on a feature.\n",
        "\n",
        "- Decision trees use Information Gain to decide:\n",
        "\n",
        "  - which feature to split on at each node.\n",
        "\n",
        "- How it works (conceptually):\n",
        "  1. Measure entropy (uncertainty) before the split.\n",
        "  2. Measure entropy after the split.\n",
        "  3. Subtract.\n",
        "\n",
        "- Information Gain = Entropy(before split) − Entropy(after split)\n",
        "\n",
        "- Higher Information Gain means:\n",
        "\n",
        "  - better separation of classes\n",
        "  - more useful feature\n",
        "\n",
        "- So, the decision tree chooses the feature that gives the highest Information Gain first.\n",
        "\n",
        "**Question 2. What is the difference between Gini Impurity and Entropy?**\n",
        "- Both are impurity measures used to decide splits in decision trees.\n",
        "\n",
        "- Write this in your notebook:\n",
        "  - Gini Impurity:\n",
        "    Measures how often a randomly chosen sample would be incorrectly classified if it were labeled randomly according to the distribution of classes.\n",
        "\n",
        "  - Entropy:\n",
        "    Measures the level of disorder or unpredictability in the data.\n",
        "\n",
        "- Now the comparison:\n",
        "\n",
        "  - Gini Impurity:\n",
        "    - Faster to compute\n",
        "    - Often preferred in CART (used in scikit-learn)\n",
        "    - Biased slightly toward dominant classes\n",
        "\n",
        "  - Entropy:\n",
        "    - Based on information theory (logarithmic)\n",
        "    - Gives more weight to rare classes\n",
        "    - Slower to compute than Gini\n",
        "\n",
        "- When to use:\n",
        "\n",
        "  - Use Gini when:\n",
        "    speed and simplicity matter.\n",
        "\n",
        "  - Use Entropy when:\n",
        "    you care more about capturing uncertainty and want more balanced splits.\n",
        "\n",
        "- Both usually produce very similar trees in practice.\n",
        "\n",
        "**Question 3. What is Pre-Pruning in Decision Trees?**\n",
        "- Pre-pruning (also called early stopping) means:\n",
        "\n",
        "  - stopping the tree from growing too deep while it is being built.\n",
        "\n",
        "- Instead of allowing the tree to grow fully and pruning later, we limit growth using rules like:\n",
        "\n",
        "  - max depth\n",
        "  - minimum samples to split\n",
        "  - minimum samples in leaf\n",
        "  - maximum number of nodes\n",
        "\n",
        "- Purpose:\n",
        "\n",
        "  - prevent overfitting\n",
        "  - improve generalization\n",
        "  - reduce training time\n",
        "\n",
        "- Example settings in scikit-learn:\n",
        "\n",
        "  - max_depth=5, min_samples_split=10, min_samples_leaf=5\n",
        "\n",
        "**Question 4. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).**"
      ],
      "metadata": {
        "id": "zs9nccFBua90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY_UN6HhtuMf",
        "outputId": "fcd87520-266c-4731-9392-360305f4e00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm) : 0.0\n",
            "sepal width (cm) : 0.013333333333333329\n",
            "petal length (cm) : 0.06405595813204505\n",
            "petal width (cm) : 0.9226107085346216\n"
          ]
        }
      ],
      "source": [
        "# Decision Tree Classifier with Gini Impurity\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train model\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, score in zip(X.columns, model.feature_importances_):\n",
        "    print(name, \":\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5.  What is a Support Vector Machine (SVM)?**\n",
        "- Support Vector Machine is a classification algorithm that:\n",
        "\n",
        "  - finds the best boundary (hyperplane) that separates classes with the maximum margin.\n",
        "\n",
        "- It focuses on the most important data points (support vectors), not all samples.\n",
        "\n",
        "- Used for:\n",
        "\n",
        "  - classification\n",
        "  - regression\n",
        "  - outlier detection\n",
        "\n",
        "- Works well on high-dimensional data.\n",
        "\n",
        "**Question 6. What is the Kernel Trick in SVM?**\n",
        "- Kernel Trick allows SVM to classify non-linear data by mapping it into higher dimensions without actually computing the transformation.\n",
        "\n",
        "- Instead, it uses kernel functions such as:\n",
        "\n",
        "  - linear\n",
        "  - polynomial\n",
        "  - RBF (Gaussian)\n",
        "\n",
        "- So a problem that is not linearly separable becomes separable.\n",
        "\n",
        "- Example:\n",
        "\n",
        "  - Linear data → linear kernel\n",
        "  - Curved boundary → RBF kernel\n",
        "\n",
        "**Question 7. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.**"
      ],
      "metadata": {
        "id": "m_2R8Vq1xbve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare SVM Linear vs RBF on Wine dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Linear SVM\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "# RBF SVM\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "print(\"Linear SVM Accuracy:\", linear_acc)\n",
        "print(\"RBF SVM Accuracy:\", rbf_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcMrqp9cx9H0",
        "outputId": "9fbef420-06d3-4da0-d369-02b9645caaf6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9814814814814815\n",
            "RBF SVM Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8.  What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "- Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem.\n",
        "\n",
        "- It assumes:\n",
        "\n",
        "  - all features are independent of each other — which is often not true.\n",
        "\n",
        "- Because of this unrealistic independence assumption, it is called “Naïve.”\n",
        "\n",
        "- Still, it works surprisingly well for:\n",
        "\n",
        "  - spam filtering\n",
        "  - document classification\n",
        "  - sentiment analysis\n",
        "\n",
        "- Fast, simple, and effective.\n",
        "\n",
        "**Question 9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.**\n",
        "\n",
        "**1. Gaussian Naïve Bayes**\n",
        "\n",
        "- Used when features are continuous and follow a normal distribution.\n",
        "\n",
        "- Example:\n",
        "  - height, age, temperature.\n",
        "\n",
        "  - sklearn.naive_bayes.GaussianNB\n",
        "\n",
        "**2. Multinomial Naïve Bayes**\n",
        "\n",
        "- Used for count-based data.\n",
        "\n",
        "- Example:\n",
        "  - word counts in text (Bag-of-Words, TF-IDF).\n",
        "\n",
        "  - sklearn.naive_bayes.MultinomialNB\n",
        "\n",
        "**3. Bernoulli Naïve Bayes**\n",
        "\n",
        "- Used for binary features (0/1).\n",
        "\n",
        "- Example:\n",
        "  - word present = 1, word absent = 0.\n",
        "\n",
        "  - sklearn.naive_bayes.BernoulliNB\n",
        "\n",
        "**Quick Summary:**\n",
        "\n",
        "  - Gaussian → continuous numeric\n",
        "  - Multinomial → word counts\n",
        "  - Bernoulli → binary presence/absence\n",
        "\n",
        "**Question 10. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.**"
      ],
      "metadata": {
        "id": "sNwHtHvVyHkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Naive Bayes on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=0\n",
        ")\n",
        "\n",
        "# Model\n",
        "model = GaussianNB()\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP2qQmV8x-jz",
        "outputId": "e12635f4-a30e-4e58-ffe1-2102d60102c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9239766081871345\n"
          ]
        }
      ]
    }
  ]
}