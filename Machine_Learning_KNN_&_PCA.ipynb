{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#KNN & PCA Assignment"
      ],
      "metadata": {
        "id": "cCY3anyI371B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "- K-Nearest Neighbors (KNN) is a lazy, distance-based machine learning algorithm.\n",
        "\n",
        "- It works by:\n",
        "  - finding the K closest data points to a new data point using a distance metric (usually Euclidean distance)\n",
        "  - making predictions based on those neighbors\n",
        "\n",
        "- In classification:\n",
        "  - the class is decided by majority voting among the K neighbors\n",
        "\n",
        "- In regression:\n",
        "  - the prediction is the average value of the K neighbors\n",
        "\n",
        "- KNN does not build a model during training; it stores the data and computes distances during prediction.\n",
        "\n",
        "**Question 2. What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "- The Curse of Dimensionality refers to problems that occur when the number of features (dimensions) becomes very large.\n",
        "\n",
        "- As dimensions increase:\n",
        "  - distances between data points become less meaningful\n",
        "  - all points appear almost equally far away\n",
        "  - KNN struggles to find true “nearest” neighbors\n",
        "\n",
        "- Effect on KNN:\n",
        "  - reduced accuracy\n",
        "  - increased computation time\n",
        "  - poor generalization\n",
        "\n",
        "- This is why KNN performs best with low-dimensional data.\n",
        "\n",
        "**Question 3. What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "- Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features into a smaller set of new features called principal components.\n",
        "\n",
        "- Key differences:\n",
        "\n",
        "- PCA:\n",
        "  - creates new features\n",
        "  - combines original features\n",
        "  - focuses on maximizing variance\n",
        "\n",
        "- Feature Selection:\n",
        "  - selects a subset of original features\n",
        "  - does not create new features\n",
        "  - focuses on relevance or importance\n",
        "\n",
        "- In short: PCA transforms features, while feature selection chooses features.\n",
        "\n",
        "**Question 4. What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "- Eigenvectors represent the directions (axes) of maximum variance in the data.\n",
        "\n",
        "- Eigenvalues represent the amount of variance captured by each eigenvector.\n",
        "\n",
        "- Importance in PCA:\n",
        "  - eigenvectors define the new feature space\n",
        "  - eigenvalues tell how much information each component holds\n",
        "  - components with larger eigenvalues are kept\n",
        "  - components with smaller eigenvalues are discarded\n",
        "\n",
        "- This helps reduce dimensions while retaining maximum information.\n",
        "\n",
        "**Question 5. How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "- PCA helps reduce the number of dimensions, which:\n",
        "  - reduces noise\n",
        "  - removes redundant features\n",
        "  - makes distances more meaningful\n",
        "\n",
        "- KNN benefits from PCA because:\n",
        "  - distance calculations become more reliable\n",
        "  - computation becomes faster\n",
        "  - accuracy often improves\n",
        "\n",
        "- Typical pipeline:\n",
        "  - Apply PCA to reduce dimensions\n",
        "  - Train KNN on transformed data\n",
        "\n",
        "- Together, PCA improves KNN’s efficiency and performance.\n",
        "\n",
        "**Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.**"
      ],
      "metadata": {
        "id": "jPjjvtsS4A17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAIVh0pF316_",
        "outputId": "0fd498bc-4ce5-4138-cbb2-d67cf5a77454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=0\n",
        ")\n",
        "\n",
        "# KNN without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "acc_without_scaling = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# KNN with scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_with_scaling = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_without_scaling)\n",
        "print(\"Accuracy with scaling:\", acc_with_scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Conclusion:** Feature scaling significantly improves KNN performance.\n",
        "\n",
        "**Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**"
      ],
      "metadata": {
        "id": "Iiu_45Hv5vY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "print(\"Explained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duRAbbn35sQD",
        "outputId": "c56807ed-41d8-4c28-e410-39a5565e0d1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Interpretation:** The first few components capture most of the variance.\n",
        "\n",
        "**Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.**"
      ],
      "metadata": {
        "id": "9t_TV8WJ6FMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca_2 = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Train-test split\n",
        "X_train_pca, X_test_pca, y_train, y_test = train_test_split(\n",
        "    X_pca_2, y, test_size=0.3, random_state=0\n",
        ")\n",
        "\n",
        "# KNN on PCA data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy on PCA-transformed data:\", acc_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc01RvJP6B5d",
        "outputId": "621daa04-d02d-439f-eabd-82d62e3a4593"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on PCA-transformed data: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Comparison:**\n",
        "  - Original scaled data: ~0.96\n",
        "  - PCA (2 components): ~0.98\n",
        "    \n",
        "    Slight accuracy loss, but much lower dimensionality.\n",
        "\n",
        "**Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.**"
      ],
      "metadata": {
        "id": "V5DUHFzG6osA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN with Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test_scaled))\n",
        "\n",
        "# KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test_scaled))\n",
        "\n",
        "print(\"Euclidean Distance Accuracy:\", acc_euclidean)\n",
        "print(\"Manhattan Distance Accuracy:\", acc_manhattan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNctmhOT6lzv",
        "outputId": "352a5d58-dd5f-49cf-b34b-6ac4fac321c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Distance Accuracy: 1.0\n",
            "Manhattan Distance Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Conclusion:** Euclidean distance performs slightly better for this dataset."
      ],
      "metadata": {
        "id": "42mYpuUI7Zmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.**\n",
        "\n",
        "**Due to the large number of features and a small number of samples, traditional models  overfit.**\n",
        "\n",
        "Explain how you would:\n",
        " - Use PCA to reduce dimensionality\n",
        " - Decide how many components to keep\n",
        " - Use KNN for classification post-dimensionality reduction\n",
        " - Evaluate the model\n",
        " - Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n"
      ],
      "metadata": {
        "id": "8L0cDuTB7iU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Simulated gene expression data\n",
        "X, y = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=500,\n",
        "    n_informative=50,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=0.95)  # keep 95% variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "scores = cross_val_score(knn, X_pca, y, cv=5)\n",
        "\n",
        "print(\"Number of PCA components:\", X_pca.shape[1])\n",
        "print(\"Cross-validated Accuracy:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0qszrhM7S8Z",
        "outputId": "cd9cffd1-892a-43b5-a914-4c71ad33bc4f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PCA components: 161\n",
            "Cross-validated Accuracy: 0.6199999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-by-Step Pipeline Explanation**\n",
        "\n",
        "- PCA for Dimensionality Reduction\n",
        "  - Scale features\n",
        "  - Apply PCA to remove noise and redundant features\n",
        "\n",
        "- Choosing Number of Components\n",
        "  - Use explained variance ratio\n",
        "  - Retain components explaining ~90–95% variance\n",
        "\n",
        "- KNN for Classification\n",
        "  - Apply KNN on reduced feature space\n",
        "  - Lower dimensions → better distance calculations\n",
        "\n",
        "- Model Evaluation\n",
        "  - Use cross-validation\n",
        "  - Metrics: accuracy, F1-score, ROC-AUC (important for medical data)\n",
        "\n",
        "- Business & Scientific Justification\n",
        "  - Reduces overfitting\n",
        "  - Improves interpretability\n",
        "  - Faster computation\n",
        "  - More reliable predictions for biomedical decisions"
      ],
      "metadata": {
        "id": "F8Gsxp0N8B1j"
      }
    }
  ]
}